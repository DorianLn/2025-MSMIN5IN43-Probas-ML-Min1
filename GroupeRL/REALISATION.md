# R√©alisation du Projet RL : Contr√¥le & Jeux

## üìã Vue d'ensemble du projet

**Objectif** : Apprendre √† un agent √† jouer √† un jeu vid√©o ou contr√¥ler un syst√®me physique en utilisant le Reinforcement Learning.

**Librairies principales** :
- **Stable-Baselines3** : Impl√©mentations modernes des algorithmes RL (PPO, DQN, SAC)
- **Gymnasium** : Environnements standardis√©s pour tester les agents RL

**Algorithmes √† comparer** : PPO, DQN, SAC

---

## üéØ √âtapes de r√©alisation

### √âtape 1 : Configuration de l'environnement

#### 1.1 Cr√©er un environnement virtuel Python
```bash
python -m venv venv
# Sur Windows
venv\Scripts\activate
```

#### 1.2 Installer les d√©pendances requises
```bash
pip install stable-baselines3 gymnasium pygame numpy matplotlib tensorboard
```

**Explication des packages** :
- `stable-baselines3` : Les algos PPO, DQN, SAC
- `gymnasium` : Les environnements de jeu
- `pygame` : Pour visualiser les jeux
- `numpy` : Calculs num√©riques
- `matplotlib` : Visualisation des r√©sultats
- `tensorboard` : Suivi de l'entra√Ænement

---

### √âtape 2 : Choisir l'environnement de test

#### Deux cat√©gories possibles :

**Option A : Jeux vid√©o (recommand√© pour d√©marrer)**
- `CartPole-v1` ‚≠ê (PLUS SIMPLE - Commencer ici)
- `LunarLander-v2` (Niveau moyen)
- `Breakout-v4` (Niveau avanc√© - n√©cessite `stable-baselines3[atari]`)

**Option B : Syst√®mes physiques (plus complexes)**
- `Pendulum-v1` (Pendule invers√©)
- `MountainCar-v0` (Voiture en montagne)

### ‚úÖ Recommandation pour d√©buter :
**Commencer avec `CartPole-v1`** (simple, rapide, bon pour les tests)

---

### √âtape 3 : Cr√©er les scripts de base

Cr√©er la structure suivante dans le dossier du projet :

```
GroupeRL/
‚îú‚îÄ‚îÄ scripts/
‚îÇ   ‚îú‚îÄ‚îÄ train_ppo.py
‚îÇ   ‚îú‚îÄ‚îÄ train_dqn.py
‚îÇ   ‚îú‚îÄ‚îÄ train_sac.py
‚îÇ   ‚îú‚îÄ‚îÄ test_agent.py
‚îÇ   ‚îî‚îÄ‚îÄ benchmark_algos.py
‚îú‚îÄ‚îÄ models/
‚îÇ   ‚îú‚îÄ‚îÄ ppo_cartpole.zip
‚îÇ   ‚îú‚îÄ‚îÄ dqn_cartpole.zip
‚îÇ   ‚îî‚îÄ‚îÄ sac_cartpole.zip
‚îú‚îÄ‚îÄ results/
‚îÇ   ‚îî‚îÄ‚îÄ comparaison_algos.png
‚îî‚îÄ‚îÄ REALISATION.md
```

---

### √âtape 4 : Entra√Æner les trois algorithmes

#### 4.1 Script d'entra√Ænement PPO

**Fichier** : `scripts/train_ppo.py`

```python
import gymnasium as gym
from stable_baselines3 import PPO
from stable_baselines3.common.callbacks import EvalCallback

# Cr√©er l'environnement
env = gym.make("CartPole-v1")

# Cr√©er le mod√®le PPO
model = PPO(
    "MlpPolicy",
    env,
    n_steps=2048,
    batch_size=64,
    n_epochs=10,
    learning_rate=3e-4,
    verbose=1,
    device="cpu"  # ou "cuda" si GPU disponible
)

# Entra√Æner
model.learn(total_timesteps=50000)

# Sauvegarder
model.save("models/ppo_cartpole")

env.close()
print("‚úÖ Entra√Ænement PPO termin√© !")
```

#### 4.2 Script d'entra√Ænement DQN

**Fichier** : `scripts/train_dqn.py`

```python
import gymnasium as gym
from stable_baselines3 import DQN

# Cr√©er l'environnement
env = gym.make("CartPole-v1")

# Cr√©er le mod√®le DQN
model = DQN(
    "MlpPolicy",
    env,
    learning_rate=1e-3,
    buffer_size=10000,
    learning_starts=1000,
    target_update_interval=500,
    verbose=1,
    device="cpu"
)

# Entra√Æner
model.learn(total_timesteps=50000)

# Sauvegarder
model.save("models/dqn_cartpole")

env.close()
print("‚úÖ Entra√Ænement DQN termin√© !")
```

#### 4.3 Script d'entra√Ænement SAC

**Fichier** : `scripts/train_sac.py`

```python
import gymnasium as gym
from stable_baselines3 import SAC

# Cr√©er l'environnement
env = gym.make("CartPole-v1")

# Cr√©er le mod√®le SAC
model = SAC(
    "MlpPolicy",
    env,
    learning_rate=3e-4,
    buffer_size=10000,
    learning_starts=100,
    verbose=1,
    device="cpu"
)

# Entra√Æner
model.learn(total_timesteps=50000)

# Sauvegarder
model.save("models/sac_cartpole")

env.close()
print("‚úÖ Entra√Ænement SAC termin√© !")
```

**√Ä FAIRE DANS CETTE √âTAPE** :
1. Cr√©er les fichiers `train_ppo.py`, `train_dqn.py`, `train_sac.py`
2. Ex√©cuter chaque script :
   ```bash
   python scripts/train_ppo.py
   python scripts/train_dqn.py
   python scripts/train_sac.py
   ```
3. Attendre que les 3 entra√Ænements se terminent (‚è±Ô∏è 5-10 minutes au total)

---

### √âtape 5 : Tester les agents entra√Æn√©s

#### 5.1 Script de test simple

**Fichier** : `scripts/test_agent.py`

```python
import gymnasium as gym
from stable_baselines3 import PPO, DQN, SAC

# Cr√©er l'environnement
env = gym.make("CartPole-v1", render_mode="human")

# Tester chaque mod√®le
models = {
    "PPO": PPO.load("models/ppo_cartpole", env=env),
    "DQN": DQN.load("models/dqn_cartpole", env=env),
    "SAC": SAC.load("models/sac_cartpole", env=env)
}

for algo_name, model in models.items():
    print(f"\nüéÆ Test de {algo_name}...")
    
    # 5 √©pisodes de test
    for episode in range(5):
        obs, info = env.reset()
        done = False
        total_reward = 0
        steps = 0
        
        while not done:
            action, _ = model.predict(obs, deterministic=True)
            obs, reward, terminated, truncated, info = env.step(action)
            done = terminated or truncated
            total_reward += reward
            steps += 1
        
        print(f"  Episode {episode+1}: Score = {total_reward:.0f}, √âtapes = {steps}")
    
    print(f"‚úÖ Tests de {algo_name} termin√©s !")

env.close()
```

**√Ä FAIRE** :
```bash
python scripts/test_agent.py
```

Vous verrez une **fen√™tre de jeu** s'afficher avec le b√¢ton qui essaie de rester √©quilibr√©. Les agents entra√Æn√©s vont contr√¥ler le mouvement du chariot.

---

### √âtape 6 : Comparer les performances

#### 6.1 Benchmark des trois algorithmes

**Fichier** : `scripts/benchmark_algos.py`

```python
import gymnasium as gym
import numpy as np
import matplotlib.pyplot as plt
from stable_baselines3 import PPO, DQN, SAC

def evaluate_agent(model, env, num_episodes=10):
    """√âvalue un agent sur plusieurs √©pisodes"""
    scores = []
    
    for _ in range(num_episodes):
        obs, _ = env.reset()
        done = False
        episode_reward = 0
        
        while not done:
            action, _ = model.predict(obs, deterministic=True)
            obs, reward, terminated, truncated, _ = env.step(action)
            done = terminated or truncated
            episode_reward += reward
        
        scores.append(episode_reward)
    
    return scores

# Cr√©er l'environnement
env = gym.make("CartPole-v1")

# Charger les mod√®les
models = {
    "PPO": PPO.load("models/ppo_cartpole"),
    "DQN": DQN.load("models/dqn_cartpole"),
    "SAC": SAC.load("models/sac_cartpole")
}

# √âvaluer tous les mod√®les
results = {}
for algo_name, model in models.items():
    print(f"√âvaluation de {algo_name}...")
    scores = evaluate_agent(model, env, num_episodes=20)
    results[algo_name] = scores
    print(f"  Moyenne: {np.mean(scores):.2f}")
    print(f"  √âcart-type: {np.std(scores):.2f}")
    print(f"  Min/Max: {np.min(scores):.0f}/{np.max(scores):.0f}")

# Afficher les r√©sultats
fig, axes = plt.subplots(1, 2, figsize=(14, 5))

# Graphique 1 : Bo√Ætes √† moustaches
axes[0].boxplot([results[algo] for algo in results.keys()], 
                labels=list(results.keys()))
axes[0].set_ylabel("Score")
axes[0].set_title("Comparaison des scores (CartPole-v1)")
axes[0].grid(True, alpha=0.3)

# Graphique 2 : Moyenne et √©cart-type
means = [np.mean(results[algo]) for algo in results.keys()]
stds = [np.std(results[algo]) for algo in results.keys()]
x = np.arange(len(results))
axes[1].bar(x, means, yerr=stds, capsize=10, alpha=0.7)
axes[1].set_xticks(x)
axes[1].set_xticklabels(list(results.keys()))
axes[1].set_ylabel("Score moyen")
axes[1].set_title("Score moyen avec intervalle de confiance")
axes[1].grid(True, alpha=0.3, axis='y')

plt.tight_layout()
plt.savefig("results/comparaison_algos.png", dpi=100)
print("\nüìä Graphique sauvegard√© dans 'results/comparaison_algos.png'")
plt.show()

env.close()
```

**√Ä FAIRE** :
```bash
python scripts/benchmark_algos.py
```

Cela g√©n√©rera un graphique comparant les trois algorithmes.

---

## üéÆ Guide complet pour tester avec des jeux

### Option 1 : Visualisation simple pendant l'entra√Ænement

**Ajouter ceci √† votre script d'entra√Ænement** :

```python
import gymnasium as gym
from stable_baselines3 import PPO

# Avec render_mode="human" pour afficher le jeu
env = gym.make("CartPole-v1", render_mode="human")

model = PPO("MlpPolicy", env, verbose=1)
model.learn(total_timesteps=10000)

env.close()
```

### Option 2 : Test interactif avec ralentissement

**Fichier** : `scripts/play_game.py`

```python
import gymnasium as gym
from stable_baselines3 import PPO
import time

env = gym.make("CartPole-v1", render_mode="human")
model = PPO.load("models/ppo_cartpole")

obs, _ = env.reset()
done = False

while not done:
    action, _ = model.predict(obs, deterministic=True)
    obs, reward, terminated, truncated, _ = env.step(action)
    done = terminated or truncated
    
    time.sleep(0.05)  # Ralentir pour mieux voir

env.close()
```

### Option 3 : Enregistrer une vid√©o

**Fichier** : `scripts/record_video.py`

```python
import gymnasium as gym
from gymnasium.wrappers import RecordVideo
from stable_baselines3 import PPO

env = gym.make("CartPole-v1")
env = RecordVideo(env, video_folder="videos/")

model = PPO.load("models/ppo_cartpole")

obs, _ = env.reset()
done = False

while not done:
    action, _ = model.predict(obs, deterministic=True)
    obs, reward, terminated, truncated, _ = env.step(action)
    done = terminated or truncated

env.close()
print("‚úÖ Vid√©o enregistr√©e dans 'videos/'")
```

### Option 4 : Tester sur plusieurs environnements diff√©rents

**Autres environnements simples √† tester** :

```python
# Essayer LunarLander (alunissage)
env = gym.make("LunarLander-v2", render_mode="human")

# Ou MountainCar (voiture en montagne)
env = gym.make("MountainCar-v0", render_mode="human")

# Ou Pendulum (pendule invers√© - continu)
env = gym.make("Pendulum-v1", render_mode="human")
```

---

## üìä R√©sum√© des √©tapes d'ex√©cution

```
1. ‚úÖ Installer Python et d√©pendances (5 min)
   ‚Üí pip install stable-baselines3 gymnasium pygame numpy matplotlib

2. ‚úÖ Entra√Æner PPO (2-3 min)
   ‚Üí python scripts/train_ppo.py

3. ‚úÖ Entra√Æner DQN (2-3 min)
   ‚Üí python scripts/train_dqn.py

4. ‚úÖ Entra√Æner SAC (2-3 min)
   ‚Üí python scripts/train_sac.py

5. ‚úÖ Tester les agents (1 min)
   ‚Üí python scripts/test_agent.py

6. ‚úÖ Comparer les r√©sultats (1-2 min)
   ‚Üí python scripts/benchmark_algos.py
   ‚Üí G√©n√©rera "results/comparaison_algos.png"

‚è±Ô∏è Temps total : 15-20 minutes
```

---

## üîç Interpr√©tation des r√©sultats

### CartPole-v1
- **Objectif** : Garder un b√¢ton en √©quilibre sur un chariot mobile
- **Score maximal** : 500 (r√©ussite compl√®te)
- **Score acceptable** : > 400

### Comment interpr√©ter les courbes :
1. **Moyenne** : Performance globale (plus haut = mieux)
2. **√âcart-type** : Stabilit√© (plus bas = plus stable)
3. **Min/Max** : Consistency (Min proche de la Moyenne = bon)

### R√©sultats typiques :
- **PPO** : Stable, performant (environ 450-500)
- **DQN** : Peut √™tre instable avec peu de donn√©es
- **SAC** : Bon √©quilibre, mais peut converger moins vite

---

## üöÄ Extensions possibles

1. **Changez d'environnement** : LunarLander, MountainCar, etc.
2. **Hyperparam√®tres** : Ajustez `learning_rate`, `n_steps`, etc.
3. **Entra√Ænement plus long** : Augmentez `total_timesteps`
4. **Atari games** : Installez `stable-baselines3[atari]` pour des jeux plus complexes
5. **Analyse** : Cr√©ez d'autres graphiques (courbes d'apprentissage, etc.)

---

## ‚ö†Ô∏è D√©pannage

| Probl√®me | Solution |
|----------|----------|
| ImportError pour gymnasium | `pip install gymnasium` |
| ImportError pour stable-baselines3 | `pip install stable-baselines3` |
| La fen√™tre de jeu ne s'affiche pas | V√©rifiez que pygame est install√© : `pip install pygame` |
| Entra√Ænement tr√®s lent | R√©duisez `total_timesteps` pour les tests rapides |
| Erreur GPU | Remplacez `device="cuda"` par `device="cpu"` |

---

## üìù Fichiers de r√©f√©rence

- [Stable-Baselines3 Docs](https://stable-baselines3.readthedocs.io/)
- [Gymnasium Documentation](https://gymnasium.farama.org/)
- [PPO Paper](https://arxiv.org/abs/1707.06347)
- [DQN Paper](https://arxiv.org/abs/1312.5602)
- [SAC Paper](https://arxiv.org/abs/1801.01290)

---

‚ú® **Bon apprentissage !**
